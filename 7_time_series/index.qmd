
## load data and packages

```{r}
pacman::p_load(
  rio, # File import
  here, # File locator
  tsibble, # handle time series datasets
  slider, # for calculating moving averages
  imputeTS, # for filling in missing values
  feasts, # for time series decomposition and autocorrelation
  forecast, # fit sin and cosin terms to data (note: must load after feasts)
  trending, # fit and assess models
  tmaptools, # for getting geocoordinates (lon/lat) based on place names
  ecmwfr, # for interacting with copernicus sateliate CDS API
  stars, # for reading in .nc (climate data) files
  units, # for defining units of measurement (climate data)
  yardstick, # for looking at model accuracy
  surveillance, # for aberration detection
  tidyverse # data management + ggplot2 graphics
)
```


```{r}
# load the data weekly counts of campylobacter cases reported in Germany
# import the counts into R
counts <- rio::import("campylobacter_germany.xlsx")
head(counts)
```

```{r}
## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>%
  mutate(epiweek = yearweek(date, week_start = 1))
head(counts)
```

```{r}
# 1. List all .nc files in the "data" directory
nc_files <- list.files("data", pattern = "\\.nc$", full.names = TRUE)

# 2. Read all files as a single stars object (create a time dimension if appropriate)
data <- read_stars(nc_files, along = "time")
```

```{r}
## change to a data frame
temp_data <- as_tibble(data) %>%
  ## add in variables and correct units
  mutate(
    ## create an calendar week variable
    epiweek = tsibble::yearweek(time),
    ## create a date variable (start of calendar week)
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    t2m = set_units(t2m, celsius),
    ## change precipitation from metres to millimetres
    tp = set_units(tp, mm)
  ) %>%
  ## group by week (keep the date too though)
  group_by(epiweek, date) %>%
  ## get the average per week
  summarise(
    t2m = as.numeric(mean(t2m)),
    tp = as.numeric(mean(tp))
  )
```


## Time Series Data

```{r}
## define time series object
counts <- tsibble(counts, index = epiweek)
head(counts)
```

```{r}
# visualize time series data
## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) +
  geom_line()
```

### duplicates

```{r}
## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek)

## get a data frame of any duplicated rows
duplicates(counts, index = epiweek)
```

### missing

```{r}
## create a variable with missings instead of weeks with reporting issues
counts <- counts %>%
  mutate(case_miss = if_else(
    ## if epiweek contains 52, 53, 1 or 2
    str_detect(epiweek, "W51|W52|W53|W01|W02"),
    ## then set to missing
    NA_real_,
    ## otherwise keep the value in case
    case
  ))

## alternatively interpolate missings by linear trend
## between two nearest adjacent points
counts <- counts %>%
  mutate(case_int = imputeTS::na_interpolation(case_miss))

## to check what values have been imputed compared to the original
ggplot_na_imputations(counts$case_miss, counts$case_int) +
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```

## Descriptive Analysis

### Moving averages

```{r}
## create a moving average variable (deals with missings)
counts <- counts %>%
  ## create the ma_4w variable
  ## slide over each row of the case variable
  mutate(ma_4wk = slider::slide_dbl(case,
    ## for each row calculate the name
    ~ mean(.x, na.rm = TRUE),
    ## use the four previous weeks
    .before = 4
  ))

## make a quick visualisation of the difference
ggplot(counts, aes(x = epiweek)) +
  geom_line(aes(y = case)) +
  geom_line(aes(y = ma_4wk), colour = "red")
```

### Periodicity

```{r}
## Function arguments
#####################
## x is a dataset
## counts is variable with count data or rates within x
## start_week is the first week in your dataset
## period is how many units in a year
## output is whether you want return spectral periodogram or the peak weeks
## "periodogram" or "weeks"

# Define function
periodogram <- function(x,
                        counts,
                        start_week = c(2002, 1),
                        period = 52,
                        output = "weeks") {
  ## make sure is not a tsibble, filter to project and only keep columns of interest
  prepare_data <- dplyr::as_tibble(x)

  # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
  prepare_data <- dplyr::select(prepare_data, {{ counts }})

  ## create an intermediate "zoo" time series to be able to use with spec.pgram
  zoo_cases <- zoo::zooreg(prepare_data,
    start = start_week, frequency = period
  )

  ## get a spectral periodogram not using fast fourier transform
  periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)

  ## return the peak weeks
  periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period

  if (output == "weeks") {
    periodo_weeks
  } else {
    periodo
  }
}

## get spectral periodogram for extracting weeks with the highest frequencies
## (checking of seasonality)
periodo <- periodogram(counts,
  case_int,
  start_week = c(2002, 1),
  output = "periodogram"
)

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity
ggplot(
  data = periodo,
  aes(x = 1 / (periodo.freq / 52), y = log(periodo.spec))
) +
  geom_line() +
  labs(x = "Period (Weeks)", y = "Log(density)")
```

## Decomposition

```{r}
## decompose the counts dataset
counts %>%
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>%
  ## extract the important information from the model
  components() %>%
  ## generate a plot
  autoplot()
```

## Autocorrelation

```{r}
## using the counts dataset
counts %>%
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>%
  ## show a plot
  autoplot()
```

```{r}
## using the counts data set
counts %>%
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>%
  ## show a plot
  autoplot()
```

```{r}
## test for independance
Box.test(counts$case_int, type = "Ljung-Box")
```

## Fitting regressions

- Most appropriate regression method is negative binomial regression for counts data

```{r}
## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>%
  fourier(K = 1)
```



```{r}
## define the model you want to fit (negative binomial)
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier
)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals
observed <- predict(fitted_model, simulate_pi = FALSE)

estimate_res <- data.frame(observed$result)

## plot your regression
ggplot(data = estimate_res, aes(x = epiweek)) +
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
    col = "Red"
  ) +
  ## add in a band for the prediction intervals
  geom_ribbon(
    aes(
      ymin = lower_pi,
      ymax = upper_pi
    ),
    alpha = 0.25
  ) +
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int),
    col = "black"
  ) +
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```

### Residuals

```{r}
## calculate the residuals
estimate_res <- estimate_res %>%
  mutate(resid = fitted_model$result[[1]]$residuals)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
estimate_res %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() +
  labs(x = "epiweek", y = "Residuals")
```

```{r}
## is there autocorelation in the residuals (is there a pattern to the error?)
estimate_res %>%
  as_tsibble(index = epiweek) %>%
  ACF(resid, lag_max = 52) %>%
  autoplot()
```

```{r}
## are residuals normally distributed (are under or over estimating?)
estimate_res %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count")
```

```{r}
## compare observed counts to their residuals
## should also be no pattern
estimate_res %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")
```

```{r}
## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence
## if p value significant then non-random
Box.test(estimate_res$resid, type = "Ljung-Box")
```

## Relation of two time series

```{r}
## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts,
  select(temp_data, -date),
  by = "epiweek"
)
head(counts)
```

```{r}
counts %>%
  ## keep the variables we are interested
  select(epiweek, case_int, t2m) %>%
  ## change your data in to long format
  pivot_longer(
    ## use epiweek as your key
    !epiweek,
    ## move column names to the new "measure" column
    names_to = "measure",
    ## move cell values to the new "values" column
    values_to = "value"
  ) %>%
  ## create a plot with the dataset above
  ## plot epiweek on the x axis and values (counts/celsius) on the y
  ggplot(aes(x = epiweek, y = value)) +
  ## create a separate plot for temperate and case counts
  ## let them set their own y-axes
  facet_grid(measure ~ ., scales = "free_y") +
  ## plot both as a line
  geom_line()
```

```{r}
counts %>%
  ## calculate cross-correlation between interpolated counts and temperature
  CCF(case_int, t2m,
    ## set the maximum lag to be 52 weeks
    lag_max = 52,
    ## return the correlation coefficient
    type = "correlation"
  ) %>%
  ## arange in decending order of the correlation coefficient
  ## show the most associated lags
  arrange(-ccf) %>%
  ## only show the top ten
  slice_head(n = 10)
```

Lags of 4 weeks is mostly highly correlated

```{r}
counts <- counts %>%
  ## create a new variable for temperature lagged by four weeks
  mutate(t2m_lag4 = lag(t2m, n = 4))
head(counts)
```

```{r}
# negative binomial with two variables
## define the model you want to fit (negative binomial)
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier +
    ## use the temperature lagged by four weeks
    t2m_lag4
)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals
observed <- predict(fitted_model, simulate_pi = FALSE)
```

```{r}
fitted_model %>%
  ## extract original negative binomial regression
  get_fitted_model() # %>%
```

```{r}
fitted_model %>%
  ## extract original negative binomial regression
  get_fitted_model() # %>%
```

```{r}
# visual inspection of the model
estimate_res <- data.frame(observed$result)

## plot your regression
ggplot(data = estimate_res, aes(x = epiweek)) +
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
    col = "Red"
  ) +
  ## add in a band for the prediction intervals
  geom_ribbon(
    aes(
      ymin = lower_pi,
      ymax = upper_pi
    ),
    alpha = 0.25
  ) +
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int),
    col = "black"
  ) +
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```

```{r}
## calculate the residuals
estimate_res <- estimate_res %>%
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
estimate_res %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() +
  labs(x = "epiweek", y = "Residuals")
```

```{r}
## is there autocorelation in the residuals (is there a pattern to the error?)
estimate_res %>%
  as_tsibble(index = epiweek) %>%
  ACF(resid, lag_max = 52) %>%
  autoplot()
```

```{r}
## are residuals normally distributed (are under or over estimating?)
estimate_res %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count")
```

```{r}
## compare observed counts to their residuals
## should also be no pattern
estimate_res %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")
```

```{r}
## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence
## if p value significant then non-random
Box.test(estimate_res$resid, type = "Ljung-Box")
```

## Outbreak Detection

Assume we are at the end of september 2011

```{r}
## define start date (when observations began)
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
cut_off <- yearweek("2010-12-31")

## define the last date interested in (i.e. end of prediction)
end_date <- yearweek("2011-12-31")

## find how many weeks in period (year) of interest
num_weeks <- as.numeric(end_date - cut_off)
```

```{r}
## add in missing weeks till end of year
counts <- counts %>%
  ## group by the region
  group_by_key() %>%
  ## for each group add rows from the highest epiweek to the end of year
  group_modify(~ add_row(.,
    epiweek = seq(max(.$epiweek) + 1,
      end_date,
      by = 1
    )
  ))
```


```{r}
## define fourier terms (sincos) 
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## (nb. 2011 fourier terms are predicted)
    fourier = rbind(
      ## get fourier terms for previous years
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = num_weeks
        )
      )
    )
```

```{r}
# split data for fitting and prediction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier
)

# define which data to use for fitting and which for predicting
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
fitted_model <- trending::fit(model, data.frame(fitting_data))

# get confint and estimates for fitted data
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# forecast with data want to predict with 
forecasts <- fitted_model %>% 
  predict(data.frame(pred_data), simulate_pi = FALSE)

## combine baseline and predicted datasets
observed <- bind_rows(observed$result, forecasts$result)
```

```{r}
## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```
### prediction validation
```{r}
## Cross validation: predicting week(s) ahead based on sliding window

## expand your data by rolling over in 52 week windows (before + after) 
## to predict 52 week ahead
## (creates longer and longer chains of observations - keeps older data)

## define window want to roll over
roll_window <- 52

## define weeks ahead want to predict 
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## label each data set with a unique id
## only use cases before year of interest (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## depending on how many weeks ahead forecasting 
    ## (otherwise will be an actual forecast to "unknown")
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## depending on what rolling window specify
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  filter(.id > roll_window)


## for each of the unique data sets run the code below
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data 
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    drop_na(case_int) %>% 
    ## get the latest week
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    pull()
  
  ## make mini_data back in to a tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    fourier = rbind(
      ## get fourier terms for previous years
      forecast::fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    case_int ~
      ## use epiweek to account for the trend
      epiweek +
      ## use the furier terms to account for seasonality
      fourier
  )

  # define which data to use for fitting and which for predicting
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  forecasts <- fitted_model %>% 
    predict(data.frame(pred_data), simulate_pi = FALSE)
  forecasts <- data.frame(forecasts$result[[1]]) %>% 
       ## only keep the week and the forecast estimate
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
  ## RMSE: Root mean squared error
  ## MAE:  Mean absolute error  
  ## MASE: Mean absolute scaled error
  ## MAPE: Mean absolute percent error
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
model_metrics
```

### surveillance package

The two methods for outbreak detection are:

- Farrington method - fits a negative binomial glm(inlcluding trend) and down weights past outbreaks to create a threshold level

- glrnb method - Fits a negative binomial glm, but inlcudes trends and fourier terms. 


```{r}
## define surveillance time series object
## nb. you can include a denominator with the population object (see ?sts)
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## subset to only keep the year from start_date 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## nb. the sts object only counts observations without assigning a week or 
## year identifier to them - so we use our data to define the appropriate observations
weekrange <- cut_off - start_date
```

```{r}
# farrington method
## define control
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline
  w = 2, ## rolling window size in weeks
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## containing the upper bound from farrington 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

```{r}
ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())
```

```{r}
# glrnb method
## define control options
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include
  trend = TRUE,   ## whether to include trend or not
  refit = FALSE), ## whether to refit model after each alarm
  ## cARL = threshold for GLR statistic (arbitrary)
     ## 3 ~ middle ground for minimising false positives
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   ret = "cases"     ## return threshold upperbound as case counts
  )

## apply the glrnb method
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## containing the upper bound from glrnb 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound
```

```{r}
ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())
```

